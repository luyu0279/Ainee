from typing import List
from langchain.prompts import PromptTemplate
from pydantic import BaseModel, Field
from app.libs.llm.llm_clients import llm_bedrock, llm

class FollowUpQuestions(BaseModel):
    """Follow-up questions generated by the model"""
    questions: List[str] = Field(description="A list of follow-up questions")

async def generate_followup_questions(context: str, question: str, last_response: str) -> List[str]:
    """
    Generate follow-up questions based on the context and original question.
    
    Args:
        context (str): The context information
        question (str): The original question
        
    Returns:
        FollowUpQuestions: A structured output containing follow-up questions and reasoning
    """
    prompt_template = """
    Analyze the conversation context and generate 3 natural follow-up questions that the user might want to ask next.

    Input Context:
    - context: {context} \n\n
    - Last user's question: {question} \n\n
    - Last AI's response: {last_response} \n\n

    Guidelines:
    1. Match user's exact speaking style, tone, and language
    2. Make questions feel like they naturally come from the user
    3. Keep questions simple and conversational

    Generate three questions that:
    1. Dig deeper into the current topic
    2. Explore related possibilities
    3. Ask about practical implementation (preferably using available tools)

    Each question should make the user think "That's exactly what I wanted to ask!"

    Constraints:
    - Must match user's language and technical level
    - No repetition of already discussed points
    - Questions should feel natural, not technical or formal
    - Most important, make sure each question is concise and clear
    """
    
    prompt = PromptTemplate(
        template=prompt_template,
        input_variables=["context", "question", "last_response"],
    )

    structure_llm = llm.with_structured_output(FollowUpQuestions)
    formated_prompt = prompt.invoke({
        "context": context,
        "question": question,
        "last_response": last_response
    })
    res = await structure_llm.ainvoke(formated_prompt)
    return res.questions
